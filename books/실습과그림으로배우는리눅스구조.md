# 실습과 그림으로 배우는 리눅스 구조

* 소스
  * https://github.com/kmin135/linux-in-practice
* 실행환경 : ubunut 16.04
* 필요한 패키지

```bash
sudo apt install binutils build-essential sysstat
```

## chap01 컴퓨터 시스템의 개요

* cpu에는 사용자 모드와 커널 모드가 있음
* 프로세스는 사용자 모드로 동작
  * 프로세스가 디바이스 드라이버 같은 커널이 제공하는 기능을 사용하려 할 때는 **시스템 콜**을 통해 커널에 요청함
* 디바이스 드라이버, 프로세스 관리 시스템, 프로세스 스케줄링, 메모리 관리 시스템등은 커널 모드에서만 동작
  * 커널 모드에서 동작하는 OS의 핵심 부분이 되는 처리를 모아 담당하는 프로그램을 커널이라고 부름



## chap02 사용자 모드로 구현되는 기능

* 시스템 콜 : 커널 모드에서만 가능한 동작을 요청하는 것
* 시스템 콜의 종류
  * 프로세스 생성, 삭제
  * 메모리 확보, 해제
  * 프로세스 간 통신 (IPC)
  * 네트워크
  * 파일시스템 다루기
  * 파일 다루기 (디바이스 접근)
* 사용자 모드에서 시스템 콜을 통하지 않고 직접 CPU를 커널 모드로 변경하는 방법은 없다.



1. 시스템콜 분석
    
    - strace 결과 각각의 줄은 1개의 시스템 콜 호출
    
    ```bash
    # -o : 분석결과를 hello.log에 저장
    # -T : 각 시스템콜의 소요시간 (마이크로초), 대신 -tt를 쓰면 ms단위로 출력
    strace -T -o hello.log ./hello
    ```
    
1. CPU 사용현황 분석
    
    - %user + %nice : 사용자 모드에서 프로세스를 실행하는 시간의 비율
    - %system : 커널 모드의 비율
- %idle : CPU 코어상에 프로세스도, 커널도 움직이고 있지 않음을 나타냄
    
    ```bash
    # 1초 단위로 cpu 코어별로 user, kernel 모드의 실행 비율을 확인할 수 있음. Ctrl-C로 나가면 평균을 출력하고 종료
    # %user, %nice, ... 각 행의 모든 필드의 합은 100
    sar -P ALL 1
    
    # 1초 단위로 2번까지 측정하고 종료
    sar -P ALL 1 2
    ```
    
1. 프로그램이 어떤 라이브러리를 링크하는가?
    ```bash
    # 많은 프로그램들이 glibc를 포함함 (libc라고 나오는 부분)
    ldd /bin/echo
    ldd /usr/bin/python3
    ```

* 고급언어로 아키텍쳐에 종속적인 시스템콜(어셈블리로 된)을 호출하기 어려우므로 OS는 시스템 콜의 wrapper 함수를 제공한다. 대표적으로 표준 C 라이브러리인 glibc가 시스템 콜 wrapper 함수를 포함함.

## chap03 프로세스 관리

* 리눅스에서 프로세스를 생성하는 두 가지 목적
  1. 같은 프로그램의 처리를 여러 개의 프로세스가 나눠서 처리한다. 예를 들면 웹 서버처럼 리퀘스트가 여러 개 들어왔을 때 동시에 처리해야 하는 경우
     - fork(), 내부적으로는 clone 시스템 콜
  2. 전혀 다른 프로그램을 생성한다. 예를 들면 bash로부터 각종 프로그램을 새로 생성하는 경우
     - execve(), 내부적으로 execve 시스템 콜
* bash 에서 echo를 실행하면 먼저 fork() 한 다음 자식 프로세스가 execve() 하는 방식으로 동작한다. 이를 `fork and exec` 방식이라 한다.
* fork 는 자식 프로세스 메모리 영역을 작성하고 거기에 부모 프로세스의 메모리를 복사한다. 그 뒤 fork의 리턴값이 부모(자식의 pid)와 자식(0)에서 다르다는 점을 이용해서 실행 처리를 분기한다.
* 반면 execve 는 현재 프로세스의 메모리를 새로운 프로세스의 데이터를 덮어씌우고 새로운 프로세스의 첫 번째 명령어부터 실행한다. 즉, 프로세스의 수가 증가하는 것이 아니고 기존의 프로세스를 별도의 프로세스로 변경하는 방식으로 수행된다. 

## chap04 프로세스 스케줄러

* P61에는 N개의 프로세스를 지정된 시간동안 실행하여 각 프로세스의 단위 시간당 경과시간과 진행도를 기록하는 프로그램 샘플이 있으니 참고

* 하나의 논리 코어에서 특정 순간에 실행되는 프로세스는 1개뿐이다.

* 시스템에 부하가 걸리면 로드밸런서가 프로세스를 여러 개의 논리 CPU에 나눠실행함. 논리 CPU 상에서 동작하는 프로세스가 바뀌는 것을 `컨텍스트 스위치` 라고 함.

* 특정 논리 CPU에서만 수행하고자 할 때는 `taskset` 명령어 사용

    ```bash
    taskset -c 0 ./onlyUseCore0
    taskset -c 0,1 ./useCore0_1
    
    # taskset은 프로세스의 실행을 특정 논리 CPU로 제한하는 sched_setaffinity() 시스템콜을 호출함
    ```

* CPU의 실행 중 혹은 실행 대기 프로세스의 수 표시

  ```bash
  # sar -q 의 필드중 runq-sz 필드는 실행 중 혹은 실행 대기 프로세스의 수를 표시
  sar -q 1 1
  ```

* 논리 CPU가 여러개이면 여러 개의 CPU를 다루기 위해 로드밸런서 또는 글로벌 스케줄러가 동작

### 프로세스의 상태

* 프로세스 상태 확인

```bash
ps ax
```

* 프로세스의 상태 종류
  * 실행 : 논리 CPU 사용중
  * 실행 대기 : CPU 시간이 할당되기를 기다리는 중
  * 슬립 상태 : 이벤트 발생 대기중, 이벤트 발생까지는 CPU 시간을 사용하지 않음.
    * 정해진 시간의 경과 대기, 키보드 등의 사용자 입력 대기, IO 장치의 읽기/쓰기 종료 대기
  * 좀비 상태 : 프로세스 종료 뒤 부모 프로세스가 종료 상태를 인식할 때까지 대기
* 3번째 필드인 STAT의 첫 문자의 의미
  * R : 실행 상태 혹은 실행 대기 상태
  * S, D : 슬립 상태. 시그널에 따라 실행 상태로 돌아오면 'S', 그렇지 않으면 'D'
    * D는 보통 저장 장치의 접근 대기를 의미함. 금방 다른 상태로 바뀌며 장시간 D에 있다면 스토리지의 I/O 문제, 커널 문제를 의심할 수 있음
  * Z : 좀비 상태
  * R : 무언가를 출력하기 위해 동작중 (예를 들면 이 예제의 `ps ax` 프로세스 자체)



### 프로세스 라이프 사이클

1. 프로세스 생성 : 프로세스 라이프사이클의 시작
2. 실행 상태 : 현재 논리 CPU를 사용하고 있음. 
   - 이벤트를 대기해야하면 (sleep, 사용자 입력, IO 등) `슬립 상태`로
   - 종료 처리를 호출하면 `좀비 상태`로
   - CPU 실행권을 잃으면 `실행 대기 상태` 로
3. 실행 대기 상태 : CPU 시간이 할당되기를 기다리고 있음
   - CPU 실행권을 얻으면 `실행 상태` 로
4. 슬립 상태 : 이벤트 발생을 기다리고 있으며 이벤트 발생까지는 CPU 시간을 사용하지 않음.
   - 이벤트가 발생하면 `실행 대기 상태`로.
5. 좀비 상태 : 프로세스 종료 후 부모 프로세스가 종료 상태를 인식할 때까지 기다리고 있음
6. 프로세스 종료 : 부모 프로세스가 종료 상태를 얻어 올바르게 종료된 상태. 라이프사이클의 끝.



### 스루풋과 레이턴시

* 스루풋 : 단위 시간당 처리된 일의 양
  * %idle이 0인데 프로세스를 늘려봐야 스루풋은 늘지 않는다. 오히려 컨텍스트 스위치 등으로 스루풋이 감소한다.
* 레이턴시 : 각각의 처리가 시작부터 종료까지의 경과된 시간
* 성능 목표를 정할 때는 구체적인 목표치를 설정하고 시스템을 튜닝해야한다.
  * 필요한 스루풋과 레이턴시 정의
  * 목표치의 예 : sar의 %idle 수치, sar -q의 runq-sz필드 (전체 논리 CPU의 실행 중 혹은 실행 대기 프로세스의 수를 표시)

### 경과 시간과 사용 시간

* 경과시간 : 프로세스 시작부터 종료까지의 시간
* 사용시간 : 프로세스가 실제로 논리 CPU를 사용한 시간 (슬립, 실행 대기 상태가 제외된 시간이라고 볼 수 있음)
* time 명령어로 얻을 수 있음

```bash
time ./someProgram

real 0m11.567s
user 0m11.560s
sys 0m0.001s

# real이 경과 시간
# user + sys 가 사용 시간. user는 사용자 모드의 CPU시간, sys는 커널이 시스템콜을 실행한 시간

# real보다 user+sys의 값이 클 수 있다. user+sys는 2개 이상의 논리 CPU를 사용할 경우 각 CPU를 사용한 시간의 합계이기 때문이다.
# 예를들어 2개의 논리CPU에 2개의 프로세스를 돌려 사용자모드100ms 만으로 모든 작업이 끝난다면 real은 100ms지만 user는 200ms가 된다.

# 반면에 sleep 명령어같이 idle 상태에 빠지는 명령어라면 real값과 비교하여 user값이 작아진다.
```

* `ps -eo` 명령어

```bash
ps -eo pid,comm,etime,time

# 순서대로 PID, COMMAND, ELAPSED, TIME
# etime이 경과시간, time이 사용시간이다.
```

* cpu 코어수 얻기

```bash
grep -c processor /proc/cpuinfo
```



### 우선순위 변경

* nice() 시스템콜로 조정
  * -19 ~ 20까지 있음. 낮을수록 우선순위가 높음.
  * 기본값은 0.
  * 우선순위를 내리는 것은 root만 가능하고 (0 미만으로 낮추는 것) 
  * 높이는 것은 아무나 가능하다 (1 이상으로 높이는 것)
* `nice` 명령어

```bash
# 우선순위 5로 실행
nice -n 5 echo hello

# sar로 살펴보면 %user가 아닌 %nice를 사용함을 볼 수 있다.
# %nice 필드는 우선순위를 디폴트 값인 0부터 변경한 프로그램을 실행한 시간의 할당량을 나타낸다. (%user는 우선순위 0인 경우)
sar -P ALL 1 
```

### 고찰

* 특정 순간 논리 CPU1개당 동작되는 프로세스는 1개뿐
* %idle이 0인 상태에서 프로세스 수를 논리 CPU 수보다 많게 해도 스루풋은 오르지 않는다.



## chap05 메모리 관리

* `free` 명령어 설명 (도식화는 p117 참조)
  * total : 시스템에 탑재된 전체 메모리 용량
  * free : 표기상 이용하지 않는 메모리
  * buff/cache : 버퍼 캐시 또는 페이지 캐시가 이용하는 메모리. free 값이 부족하면 커널이 해제함
  * available : 실질적으로 사용 가능한 메모리. free 필드값 메모리가 부족하면 해제되는 커널 내의 메모리 영역 사이즈를 더한 값. 버퍼 캐시나 페이지 캐시의 대부분 혹은 다른 커널 내의 메모리 일부가 포함됨.



* `sar -r 1` 와 같이 1초 간격으로 메모리 통계 정보를 얻을 수 있음. free와의 비교는 아래와 같음.
  * free 의 total : sar에는 없음
  * free 의 free : kbmemfree
  * free 의 buff/cache : kbbuffers + kbcached
  * available : 없음



* 더 이상 확보할 메모리가 없으면 Out Of Memory (OOM) 상태가 된다. 이 때 메모리 관리 시스템이 취하는 기본 행동은 적절한 프로세스를 선택해 강제 종료하여 메모리 확보를 시도하는 OOM Killer 라는 기능이 있다. 운영시스템이라면 당연히 치명적이다.
  * 특정 프로세스에는 OOM Killer 가 동작하지 않게도 할 수 있으나 현실적으로 어떤 프로세스는 죽어도 된다고 확신하는 것은 어렵다.
  * 특정 프로세스를 죽이는게 아니라 시스템 전체를 강제 종료하는 방법도 있다. sysctl 의 `vm.panic_on_oom` 값의 기본값 0이 OOM Killer 동작이며, 1로 변경하면 메모리 부족시 서버가 강제 종료된다.



* 커널이 프로세스에 메모리를 할당하는 두 가지 타이밍
  * 프로세스를 생성할 때
  * 프로세스를 생성한 뒤 추가로 동적 메모리를 할당할 때



* 가상 메모리
  * 절대 주소로 메모리를 할당하는 방법은 메모리 단편화, 다른 프로세스의 메모리에 잘못된 접근등의 문제가 있음.
  * 때문에 가상 메모리를 통해 프로세스가 간접적으로 메모리에 접근하도록 함
  * 프로세스에 보이는 메모리 주소를 가상 주소, 실제 메모리의 주소를 물리 주소라 함. (p125)
  * 프로세스가 물리 주소에 직접 접근하는 방법은 없음.



* 페이지 테이블
  * 가상 주소에서 물리 주소로의 변환은 커널 내부의 페이지 테이블을 사용함.
  * 가상 메모리는 전체 메모리를 페이지라는 단위로 나눠 관리하고 변환은 페이지 단위로 발생함.
  * 페이지 테이블에 한 페이지에 대한 데이터를 `페이지 테이블 엔트리`라 부름. 여기에 가상 주소와 물리 주소의 대응 정보가 들어 있음.
  * 구현 상의 이유로 커널의 메모리 일부도 프로세스의 가상 주소 공간에 매핑됨. 하지만 이 공간은 커널 모드로 실행될 때만 접근이 가능한 '커널 모드 전용' 이라는 정보가 추가되어 있으므로 사용자 모드로 동작하는 프로세스는 접근할 수 없음
  * 페이지 사이즈는 CPU 아키텍쳐마다 다름. x86_64는 4KB.
* 범위를 벗어난 가상 메모리에 접근을 시도하면? Segmentation fault!
  1. 프로세스가 범위를 벗어난 가상 주소에 접근 시도
  2. CPU에 page fault 라는 인터럽트 발생
  3. page fault에 의해 현재 실행중인 명령이 중단되고 커널 내의 page fault handler 라는 인터럽트 핸들러가 동작
  4. 커널은 프로세스로부터 메모리 접근이 잘못되었다는 내용을 page fault handler에 통지
  5. 프로세스에 SIGSEGV 시그널 통지. 이 시그널을 받은 프로세스는 강제 종료



* c 에서의 메모리 할당
  * glibc의 malloc() 함수를 사용하는데 리눅스에는 내부적으로 mmap() 함수를 호출하여 구현함
  * mmap() 함수는 페이지 단위로 메모리를 확보하고 malloc() 함수는 바이트 단위로 메모리를 확보함. 내부적으로는 glibc가 mmap() 시스템 콜로 커다란 메모리 풀을 확보함. 그 뒤 malloc()이 호출되면 메모리 풀에서 필요한 만큼 바이트 단위로 잘라내어 반환하는 처리를 함. 풀이 모자라게 되면 다시 mmap()을 호출함.
  * 이 때문에 메모리 사용량의 측정결과가 상이한 경우가 있음. 리눅스는 사용중인 메모리의 양을 측정할 때 프로세스가 생성될 때와 mmap() 함수를 호출했을 때 할당한 메모리를 전부 더한 값을 나타냄. 반면에 메모리 사용량 측정 프로그램들은 malloc() 함수 등으로 획득한 바이트 수의 총합을 나타내기 때문에 서로 다르게 표시됨. (일반적으로 리눅스가 측정한 값이 더 큼.)
* 파이썬같이 직접 메모리 관리하지 않는 언어들의 오브젝트 생성에도 최종적으로는 C언어의 malloc() 함수를 사용함.



### 가상 메모리 응용



* 파일 맵
  * 프로세스가 파일에 접근할 때는 read, write, lseek 등의 시스템 콜을 사용.
  * 파일 맵은 파일의 영역을 가상 주소 공간에 메모리 맵핑하는 기능
  * 이것도 mmap() 함수가 함.
  * 매핑된 파일은 메모리 접근과 같은 방식으로 접근이 가능함. 접근한 영역은 적절한 타이밍에 저장 장치 내의 파일에 써짐.
  * 이 원리를 이용하면 write() 시스템콜을 안 쓰고 memcpy() 와 같은 메모리 조작 함수만으로도 파일의 내용을 바꿀 수 있음. (p148)
* 디멘드 페이징
  * 필요한 메모리를 확보하고 이를 한 번에 페이지 테이블을 설정해 가상 주소 공간을 물리 주소 공간에 매핑하는건 메모리를 낭비한다. 실제로는 확보만 하고 한 번도 사용되지 않는 영역도 있기 때문.
  * demand paging은 일종의 lazy loading 방식으로 해당 페이지에 처음 접근할 때 실제 메모리를 할당함.
  * 내부적으로는 최초 접근할 때 가상 주소만 있고 물리 주소가 없으므로 페이지 폴트가 발생하고 커널이 물리 메모리를 확보하여 가상 주소와 매핑하는 과정을 거침. 사용자 프로세스는 이런 사실을 의식하지 않고 사용하게됨.
  * mmap 함수로 메모리를 확보하는 것은 '가상 메모리를 확보했음'을 의미하고 가상 메모리에 접근하여 물리 메모리를 확보하고 매핑하는 것을 '물리 메모리를 확보했음'이라고 함. 따라서 mmap이 성공하더라도 물리 메모리가 충분하지 않으면 물리 메모리 부족이 발생할 수 있음.
  * 실험 : p154
  * `sar -B 1` 로 시스템의 페이지 폴트 현황을 1초마다 볼 수 있음
  * 프로세스 별로 보기 : ps -eo pid,comm,vsz,rss,maj_flt,min_flt
    * vsz : 가상 메모리
    * rss : 물리 메모리
    * maj_flt + min_flt = 페이지 폴트 수 (메이저 폴트 수 + 마이너 폴트 수)
* 가상 메모리 부족과 물리 메모리 부족
  * 가상 메모리 부족 : 가상 주소 공간을 모두 사용하고 있어 물리 메모리가 남아 있는지 여부와 관계없이 가상 메모리를 확보할 수 없는 상태. x86 은 가상 주소 공간 4GB 였음. x86_64 는 128TB.
  * 물리 메모리 부족 : 말그대도 시스템에 탑재된 물리 메모리가 부족한 상태. 이 또한 가상 메모리가 얼마나 남아 있는지와 상관없이 발생 가능.
* Copy on Write (CoW)
  * fork 시스템콜은 가상 메모리 방식을 사용해서 고속화됨.
  * fork를 수행할 때 부모 프로세스의 메모리를 자식 프로세스에 전부 복사하는것이 아니고 페이지 테이블만 복사함.
    * 즉, 메모리 100MB 쓰는 프로세스를 fork한다고 즉각 자식 프로세스가 100MB를 먹는게 아니라 페이지 테이블 사이즈만큼만 메모리를 더 사용하게 됨.
  * 이 때, 부모와 자식 프로세스의 전체 페이지에 쓰기 권한을 무효화함.
  * 따라서 읽기만 한다면 부모 자식 모두 공유된 물리 메모리에 접근할 수 있음.
  * 하지만 어느 한 쪽이 페이지를 변경하려고 하면(쓰기) 공유를 해제함
    1. 페이지에 쓰기가 금지된 상태이므로 CPU에 페이지 폴트 발생
    2. CPU가 커널 모드로 변경되어 커널의 페이지 폴트 핸들러가 동작
    3. 페이지 폴트 핸들러는 접근한 페이지를 다른 장소에 복사하고, 쓰려고 한 프로세스에 할당한 후 내용을 다시 작성
    4. 부모 프로세스, 자식 프로세스 각각 공유가 해제된 페이지에 대응하는 페이지 테이블 엔트리 업데이트. 공유가 해제되었으므로 어느 쪽에서도 해당 엔트리에 쓰기가 가능해짐.
  * CoW의 원리에 의해 fork가 성공하더라도 페이지 폴트 핸들러가 동작하는 시점에 물리 메모리가 없으면 물리 메모리 부족이 발생할 수 있음.
  * 물리 메모리 사용량의 경우 부모와 자식의 메모리 중 공유된 부분은 각각 프로세스에 이중으로 계산됨. 이 때문에 전체 프로세스의 물리 메모리 사용량을 모두 더하면 전체 프로세스가 실제로 사용하고 있는 메모리의 양보다 더 큰 값을 가지게 됨.
* 스왑(swap)
  * 스왑은 저장 장치의 일부를 일시적으로 메모리 대신 사용하는 방식
  * OOM에 대처하는 구제 장치라 할 수 있꼬 가상 메모리 방식을 응용한 것임.
  * Windows에서의 스왑 영역은 '가상 메모리'를 의미함으로 헷갈리지 말자.
  * 임시 보관한 페이지의 스왑 영역의 위치는 커널 내의 테이블 엔트리와는 별도로 스왑 영역 관리용 장소에 기록된다.
  * 물리 메모리에 빈 공간이 없으면 커널은 사용 중인 물리 메모리의 일부를 스왑 영역에 임시 보관한다. 이를 `swap out` 이라 한다. 스왑 아웃할 영역은 커널이 특정한 알고리즘을 통해 결정한다.
  * 커널이 스왑 영역에 임시 보관했던 데이터를 물리 메모리에 되돌리는 것을 `swap in` 이라 한다.
  * 스왑 인, 스왑 아웃을 합쳐 스와핑 (swapping) 이라 부른다. 리눅스는 스왑의 단위가 페이지 단위 이므로 페이징(paging) 이라고도 부른다. 이 꼉우 스왑 인, 스왑 아웃도 페이지 인, 페이지 아웃이라 부른다.
  * 저장 장치는 메모리에 비하면 매우 느리며 시스템의 메모리 부족이 만성적으로 부족하면 메모리에 접근할 때마다 스와핑을 반복하는데 이를 스래싱(thrashing) 이라 한다. 스래싱이 나타나면 시스템이 사용자 입력에 한동안 반응하지 못 하거나 결국 컴퓨터가 멈추거나 메모리가 부족하게 된다. 따라서 스와핑은 발생하지 않는게 최선이다.
  * 스왑 확인
    * `swapon --show`
    * `free` 의 Swap: 영역
    * `sar -W 1` 1초마다 시스템의 스왑인, 스왑아웃의 수 출력. (pswpin/s : 1초당 스왑 인 된 페이지 수, pswpout/s : 1초당 스왑 아웃된 페이지 수)
    * `sar -S 1` : 1초마다 스왑 영역의 사이즈 확인. `kbswpused` 필드값이 증가하고 있으면 위험한 것.
  * Major Fault : 스와핑과 같이 저장 장치에 대한 접근이 발생하는 페이지 폴트.
  * Minor Fault : 저장 장치 접근이 발생하지 않는 페이지 폴트
* 계층형 페이지 테이블
  * x86_64 아키텍처의 가상 주소 공간 크기는 128TB, 1페이지 크기 4KB, 페이지 테이블 엔트리 사이즈 8KB
  * 페이지 테이블을 1:1로 단순 매핑해서 만들면 프로세스 1개당 페이지 테이블 사이즈로 256GB가 필요함. (8bytes * 128TB / 4KB)
  * 이를 해결하기 위해 계층구조로 페이지 테이블을 생성하여 페이지 테이블의 메모리 사용량을 절약함. x86_64는 4계층 구조라고 함.
  * 물리 메모리 중 페이지 테이블 자체가 사용하는 양은 `sar -r ALL` 의 `kbpgtbl` 필드로 확인
  * 페이지 테이블 자체도 메모리를 사용하므로 메모리 부족의 이유는 프로세스가 직접 사용하는 물리 메모리양의 증가 뿐만 아니라 '프로세스를 너무 많이 만들었다' 거나 '가상 메모리를 대량으로 사용하는 프로세스 때문에' 페이지 테이블 영역이 커져서인 경우도 있을 수 있다.
    * 전자는 프로세스의 수를 줄이는 방법을 검토하고
    * 후자는  Huge Page로 해결한다.
* Huge Page
  * 프로세스가 사용하는 가상 메모리 사이즈가 증가하면 페이지 테이블 자체의 물리 메모리양도 증가한다.
  * 이 경우 메모리 사용량 증가는 물론이고 fork 시스템 콜도 느려진다. Copy On Write 방식으로 최소한의 메모리만 복사하지만 페이지 테이블 자체는 부모 프로세스와 같은 사이즈로 새로 복사해야하기 때문이다.
  * Huge Page는 이름 대로 커다란 사이즈의 페이지이며 프로세스의 페이지 테이블에 필요한 메모리의 양을 줄일 수 있다.
  * p182를 보면 한 개의 페이지 테이블 엔트리가 매핑하는 단위 자체를 크게 해서 페이지 테이블 엔트리 수를 줄임으로써 페이지 테이블 자체의 용량을 줄이는 기법이다. 아무튼 가상 메모리를 많이 쓰는 프로세스에 대해 페이지 테이블에 필요한 메모리의 양을 줄이기 위한 기법이라는 것만 기억하자.
  * 리눅스는 Transparent Huge Page 라는 기능이 있다. 가상 주소 공간에 연속된 여러 개의 4KB 페이지가 특정 조건을 만족하면 이것을 묶어 자동으로 Huge Page로 바꿔주는 기능이다. 언뜻 좋아보이지만 Huge Page로 묶는 처리, 조건이 더 이상 만족하지 않아 다시 4KB 페이지로 풀어야할 경우의 처리가 있어 국소적으로 성능이 하락하는 경우도 있다.
    * `/sys/kernel/mm/transparent_hugepage/enabled` 파일에서 확인 가능. (우분투 20.04에서 볼 때 기본 활성화)
    * always가 활성화. 파일에 never 라고 덮어씌우면 비활성화. madvise는 madvise() 시스템 콜을 사용하여 특정 메모리 영역에만 이 기능을 사용하도록 함.



## 소감

* 당장 어디에 써먹을 수는 없으나 리눅스의 기본 동작 원리 파악에 도움이 됨. 좀 더 low level로 생각할 수 있도록 해주는 책.
* OS적인 트러블슈팅을 할 때 큰 도움이 될 듯.