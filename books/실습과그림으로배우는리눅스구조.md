# 실습과 그림으로 배우는 리눅스 구조

* 소스
  * https://github.com/kmin135/linux-in-practice
* 실행환경 : ubunut 16.04
* 필요한 패키지

```bash
sudo apt install binutils build-essential sysstat
```

## chap01 컴퓨터 시스템의 개요

* cpu에는 사용자 모드와 커널 모드가 있음
* 프로세스는 사용자 모드로 동작
  * 프로세스가 디바이스 드라이버 같은 커널이 제공하는 기능을 사용하려 할 때는 **시스템 콜**을 통해 커널에 요청함
* 디바이스 드라이버, 프로세스 관리 시스템, 프로세스 스케줄링, 메모리 관리 시스템등은 커널 모드에서만 동작
  * 커널 모드에서 동작하는 OS의 핵심 부분이 되는 처리를 모아 담당하는 프로그램을 커널이라고 부름



## chap02 사용자 모드로 구현되는 기능

* 시스템 콜 : 커널 모드에서만 가능한 동작을 요청하는 것
* 시스템 콜의 종류
  * 프로세스 생성, 삭제
  * 메모리 확보, 해제
  * 프로세스 간 통신 (IPC)
  * 네트워크
  * 파일시스템 다루기
  * 파일 다루기 (디바이스 접근)
* 사용자 모드에서 시스템 콜을 통하지 않고 직접 CPU를 커널 모드로 변경하는 방법은 없다.



1. 시스템콜 분석
   
    - strace 결과 각각의 줄은 1개의 시스템 콜 호출
    
    ```bash
    # -o : 분석결과를 hello.log에 저장
    # -T : 각 시스템콜의 소요시간 (마이크로초), 대신 -tt를 쓰면 ms단위로 출력
    strace -T -o hello.log ./hello
    ```
    
1. CPU 사용현황 분석
   
    - %user + %nice : 사용자 모드에서 프로세스를 실행하는 시간의 비율
    - %system : 커널 모드의 비율
- %idle : CPU 코어상에 프로세스도, 커널도 움직이고 있지 않음을 나타냄
  
    ```bash
    # 1초 단위로 cpu 코어별로 user, kernel 모드의 실행 비율을 확인할 수 있음. Ctrl-C로 나가면 평균을 출력하고 종료
    # %user, %nice, ... 각 행의 모든 필드의 합은 100
    sar -P ALL 1
    
    # 1초 단위로 2번까지 측정하고 종료
    sar -P ALL 1 2
    ```
    
1. 프로그램이 어떤 라이브러리를 링크하는가?
    ```bash
    # 많은 프로그램들이 glibc를 포함함 (libc라고 나오는 부분)
    ldd /bin/echo
    ldd /usr/bin/python3
    ```

* 고급언어로 아키텍쳐에 종속적인 시스템콜(어셈블리로 된)을 호출하기 어려우므로 OS는 시스템 콜의 wrapper 함수를 제공한다. 대표적으로 표준 C 라이브러리인 glibc가 시스템 콜 wrapper 함수를 포함함.

## chap03 프로세스 관리

* 리눅스에서 프로세스를 생성하는 두 가지 목적
  1. 같은 프로그램의 처리를 여러 개의 프로세스가 나눠서 처리한다. 예를 들면 웹 서버처럼 리퀘스트가 여러 개 들어왔을 때 동시에 처리해야 하는 경우
     - fork(), 내부적으로는 clone 시스템 콜
  2. 전혀 다른 프로그램을 생성한다. 예를 들면 bash로부터 각종 프로그램을 새로 생성하는 경우
     - execve(), 내부적으로 execve 시스템 콜
* bash 에서 echo를 실행하면 먼저 fork() 한 다음 자식 프로세스가 execve() 하는 방식으로 동작한다. 이를 `fork and exec` 방식이라 한다.
* fork 는 자식 프로세스 메모리 영역을 작성하고 거기에 부모 프로세스의 메모리를 복사한다. 그 뒤 fork의 리턴값이 부모(자식의 pid)와 자식(0)에서 다르다는 점을 이용해서 실행 처리를 분기한다.
* 반면 execve 는 현재 프로세스의 메모리를 새로운 프로세스의 데이터를 덮어씌우고 새로운 프로세스의 첫 번째 명령어부터 실행한다. 즉, 프로세스의 수가 증가하는 것이 아니고 기존의 프로세스를 별도의 프로세스로 변경하는 방식으로 수행된다. 

## chap04 프로세스 스케줄러

* P61에는 N개의 프로세스를 지정된 시간동안 실행하여 각 프로세스의 단위 시간당 경과시간과 진행도를 기록하는 프로그램 샘플이 있으니 참고

* 하나의 논리 코어에서 특정 순간에 실행되는 프로세스는 1개뿐이다.

* 시스템에 부하가 걸리면 로드밸런서가 프로세스를 여러 개의 논리 CPU에 나눠실행함. 논리 CPU 상에서 동작하는 프로세스가 바뀌는 것을 `컨텍스트 스위치` 라고 함.

* 특정 논리 CPU에서만 수행하고자 할 때는 `taskset` 명령어 사용

    ```bash
    taskset -c 0 ./onlyUseCore0
    taskset -c 0,1 ./useCore0_1
    
    # taskset은 프로세스의 실행을 특정 논리 CPU로 제한하는 sched_setaffinity() 시스템콜을 호출함
    ```

* CPU의 실행 중 혹은 실행 대기 프로세스의 수 표시

  ```bash
  # sar -q 의 필드중 runq-sz 필드는 실행 중 혹은 실행 대기 프로세스의 수를 표시
  sar -q 1 1
  ```

* 논리 CPU가 여러개이면 여러 개의 CPU를 다루기 위해 로드밸런서 또는 글로벌 스케줄러가 동작

### 프로세스의 상태

* 프로세스 상태 확인

```bash
ps ax
```

* 프로세스의 상태 종류
  * 실행 : 논리 CPU 사용중
  * 실행 대기 : CPU 시간이 할당되기를 기다리는 중
  * 슬립 상태 : 이벤트 발생 대기중, 이벤트 발생까지는 CPU 시간을 사용하지 않음.
    * 정해진 시간의 경과 대기, 키보드 등의 사용자 입력 대기, IO 장치의 읽기/쓰기 종료 대기
  * 좀비 상태 : 프로세스 종료 뒤 부모 프로세스가 종료 상태를 인식할 때까지 대기
* 3번째 필드인 STAT의 첫 문자의 의미
  * R : 실행 상태 혹은 실행 대기 상태
  * S, D : 슬립 상태. 시그널에 따라 실행 상태로 돌아오면 'S', 그렇지 않으면 'D'
    * D는 보통 저장 장치의 접근 대기를 의미함. 금방 다른 상태로 바뀌며 장시간 D에 있다면 스토리지의 I/O 문제, 커널 문제를 의심할 수 있음
  * Z : 좀비 상태
  * R : 무언가를 출력하기 위해 동작중 (예를 들면 이 예제의 `ps ax` 프로세스 자체)



### 프로세스 라이프 사이클

1. 프로세스 생성 : 프로세스 라이프사이클의 시작
2. 실행 상태 : 현재 논리 CPU를 사용하고 있음. 
   - 이벤트를 대기해야하면 (sleep, 사용자 입력, IO 등) `슬립 상태`로
   - 종료 처리를 호출하면 `좀비 상태`로
   - CPU 실행권을 잃으면 `실행 대기 상태` 로
3. 실행 대기 상태 : CPU 시간이 할당되기를 기다리고 있음
   - CPU 실행권을 얻으면 `실행 상태` 로
4. 슬립 상태 : 이벤트 발생을 기다리고 있으며 이벤트 발생까지는 CPU 시간을 사용하지 않음.
   - 이벤트가 발생하면 `실행 대기 상태`로.
5. 좀비 상태 : 프로세스 종료 후 부모 프로세스가 종료 상태를 인식할 때까지 기다리고 있음
6. 프로세스 종료 : 부모 프로세스가 종료 상태를 얻어 올바르게 종료된 상태. 라이프사이클의 끝.



### 스루풋과 레이턴시

* 스루풋 : 단위 시간당 처리된 일의 양
  * %idle이 0인데 프로세스를 늘려봐야 스루풋은 늘지 않는다. 오히려 컨텍스트 스위치 등으로 스루풋이 감소한다.
* 레이턴시 : 각각의 처리가 시작부터 종료까지의 경과된 시간
* 성능 목표를 정할 때는 구체적인 목표치를 설정하고 시스템을 튜닝해야한다.
  * 필요한 스루풋과 레이턴시 정의
  * 목표치의 예 : sar의 %idle 수치, sar -q의 runq-sz필드 (전체 논리 CPU의 실행 중 혹은 실행 대기 프로세스의 수를 표시)

### 경과 시간과 사용 시간

* 경과시간 : 프로세스 시작부터 종료까지의 시간
* 사용시간 : 프로세스가 실제로 논리 CPU를 사용한 시간 (슬립, 실행 대기 상태가 제외된 시간이라고 볼 수 있음)
* time 명령어로 얻을 수 있음

```bash
time ./someProgram

real 0m11.567s
user 0m11.560s
sys 0m0.001s

# real이 경과 시간
# user + sys 가 사용 시간. user는 사용자 모드의 CPU시간, sys는 커널이 시스템콜을 실행한 시간

# real보다 user+sys의 값이 클 수 있다. user+sys는 2개 이상의 논리 CPU를 사용할 경우 각 CPU를 사용한 시간의 합계이기 때문이다.
# 예를들어 2개의 논리CPU에 2개의 프로세스를 돌려 사용자모드100ms 만으로 모든 작업이 끝난다면 real은 100ms지만 user는 200ms가 된다.

# 반면에 sleep 명령어같이 idle 상태에 빠지는 명령어라면 real값과 비교하여 user값이 작아진다.
```

* `ps -eo` 명령어

```bash
ps -eo pid,comm,etime,time

# 순서대로 PID, COMMAND, ELAPSED, TIME
# etime이 경과시간, time이 사용시간이다.
```

* cpu 코어수 얻기

```bash
grep -c processor /proc/cpuinfo
```



### 우선순위 변경

* nice() 시스템콜로 조정
  * -19 ~ 20까지 있음. 낮을수록 우선순위가 높음.
  * 기본값은 0.
  * 우선순위를 내리는 것은 root만 가능하고 (0 미만으로 낮추는 것) 
  * 높이는 것은 아무나 가능하다 (1 이상으로 높이는 것)
* `nice` 명령어

```bash
# 우선순위 5로 실행
nice -n 5 echo hello

# sar로 살펴보면 %user가 아닌 %nice를 사용함을 볼 수 있다.
# %nice 필드는 우선순위를 디폴트 값인 0부터 변경한 프로그램을 실행한 시간의 할당량을 나타낸다. (%user는 우선순위 0인 경우)
sar -P ALL 1 
```

### 고찰

* 특정 순간 논리 CPU1개당 동작되는 프로세스는 1개뿐
* %idle이 0인 상태에서 프로세스 수를 논리 CPU 수보다 많게 해도 스루풋은 오르지 않는다.



## chap05 메모리 관리

* `free` 명령어 설명 (도식화는 p117 참조)
  * total : 시스템에 탑재된 전체 메모리 용량
  * free : 표기상 이용하지 않는 메모리
  * buff/cache : 버퍼 캐시 또는 페이지 캐시가 이용하는 메모리. free 값이 부족하면 커널이 해제함
  * available : 실질적으로 사용 가능한 메모리. free 필드값 메모리가 부족하면 해제되는 커널 내의 메모리 영역 사이즈를 더한 값. 버퍼 캐시나 페이지 캐시의 대부분 혹은 다른 커널 내의 메모리 일부가 포함됨.



* `sar -r 1` 와 같이 1초 간격으로 메모리 통계 정보를 얻을 수 있음. free와의 비교는 아래와 같음.
  * free 의 total : sar에는 없음
  * free 의 free : kbmemfree
  * free 의 buff/cache : kbbuffers + kbcached
  * available : 없음



* 더 이상 확보할 메모리가 없으면 Out Of Memory (OOM) 상태가 된다. 이 때 메모리 관리 시스템이 취하는 기본 행동은 적절한 프로세스를 선택해 강제 종료하여 메모리 확보를 시도하는 OOM Killer 라는 기능이 있다. 운영시스템이라면 당연히 치명적이다.
  * 특정 프로세스에는 OOM Killer 가 동작하지 않게도 할 수 있으나 현실적으로 어떤 프로세스는 죽어도 된다고 확신하는 것은 어렵다.
  * 특정 프로세스를 죽이는게 아니라 시스템 전체를 강제 종료하는 방법도 있다. sysctl 의 `vm.panic_on_oom` 값의 기본값 0이 OOM Killer 동작이며, 1로 변경하면 메모리 부족시 서버가 강제 종료된다.



* 커널이 프로세스에 메모리를 할당하는 두 가지 타이밍
  * 프로세스를 생성할 때
  * 프로세스를 생성한 뒤 추가로 동적 메모리를 할당할 때



* 가상 메모리
  * 절대 주소로 메모리를 할당하는 방법은 메모리 단편화, 다른 프로세스의 메모리에 잘못된 접근등의 문제가 있음.
  * 때문에 가상 메모리를 통해 프로세스가 간접적으로 메모리에 접근하도록 함
  * 프로세스에 보이는 메모리 주소를 가상 주소, 실제 메모리의 주소를 물리 주소라 함. (p125)
  * 프로세스가 물리 주소에 직접 접근하는 방법은 없음.



* 페이지 테이블
  * 가상 주소에서 물리 주소로의 변환은 커널 내부의 페이지 테이블을 사용함.
  * 가상 메모리는 전체 메모리를 페이지라는 단위로 나눠 관리하고 변환은 페이지 단위로 발생함.
  * 페이지 테이블에 한 페이지에 대한 데이터를 `페이지 테이블 엔트리`라 부름. 여기에 가상 주소와 물리 주소의 대응 정보가 들어 있음.
  * 구현 상의 이유로 커널의 메모리 일부도 프로세스의 가상 주소 공간에 매핑됨. 하지만 이 공간은 커널 모드로 실행될 때만 접근이 가능한 '커널 모드 전용' 이라는 정보가 추가되어 있으므로 사용자 모드로 동작하는 프로세스는 접근할 수 없음
  * 페이지 사이즈는 CPU 아키텍쳐마다 다름. x86_64는 4KB.
* 범위를 벗어난 가상 메모리에 접근을 시도하면? Segmentation fault!
  1. 프로세스가 범위를 벗어난 가상 주소에 접근 시도
  2. CPU에 page fault 라는 인터럽트 발생
  3. page fault에 의해 현재 실행중인 명령이 중단되고 커널 내의 page fault handler 라는 인터럽트 핸들러가 동작
  4. 커널은 프로세스로부터 메모리 접근이 잘못되었다는 내용을 page fault handler에 통지
  5. 프로세스에 SIGSEGV 시그널 통지. 이 시그널을 받은 프로세스는 강제 종료



* c 에서의 메모리 할당
  * glibc의 malloc() 함수를 사용하는데 리눅스에는 내부적으로 mmap() 함수를 호출하여 구현함
  * mmap() 함수는 페이지 단위로 메모리를 확보하고 malloc() 함수는 바이트 단위로 메모리를 확보함. 내부적으로는 glibc가 mmap() 시스템 콜로 커다란 메모리 풀을 확보함. 그 뒤 malloc()이 호출되면 메모리 풀에서 필요한 만큼 바이트 단위로 잘라내어 반환하는 처리를 함. 풀이 모자라게 되면 다시 mmap()을 호출함.
  * 이 때문에 메모리 사용량의 측정결과가 상이한 경우가 있음. 리눅스는 사용중인 메모리의 양을 측정할 때 프로세스가 생성될 때와 mmap() 함수를 호출했을 때 할당한 메모리를 전부 더한 값을 나타냄. 반면에 메모리 사용량 측정 프로그램들은 malloc() 함수 등으로 획득한 바이트 수의 총합을 나타내기 때문에 서로 다르게 표시됨. (일반적으로 리눅스가 측정한 값이 더 큼.)
* 파이썬같이 직접 메모리 관리하지 않는 언어들의 오브젝트 생성에도 최종적으로는 C언어의 malloc() 함수를 사용함.



### 가상 메모리 응용



* 파일 맵
  * 프로세스가 파일에 접근할 때는 read, write, lseek 등의 시스템 콜을 사용.
  * 파일 맵은 파일의 영역을 가상 주소 공간에 메모리 맵핑하는 기능
  * 이것도 mmap() 함수가 함.
  * 매핑된 파일은 메모리 접근과 같은 방식으로 접근이 가능함. 접근한 영역은 적절한 타이밍에 저장 장치 내의 파일에 써짐.
  * 이 원리를 이용하면 write() 시스템콜을 안 쓰고 memcpy() 와 같은 메모리 조작 함수만으로도 파일의 내용을 바꿀 수 있음. (p148)
* 디멘드 페이징
  * 필요한 메모리를 확보하고 이를 한 번에 페이지 테이블을 설정해 가상 주소 공간을 물리 주소 공간에 매핑하는건 메모리를 낭비한다. 실제로는 확보만 하고 한 번도 사용되지 않는 영역도 있기 때문.
  * demand paging은 일종의 lazy loading 방식으로 해당 페이지에 처음 접근할 때 실제 메모리를 할당함.
  * 내부적으로는 최초 접근할 때 가상 주소만 있고 물리 주소가 없으므로 페이지 폴트가 발생하고 커널이 물리 메모리를 확보하여 가상 주소와 매핑하는 과정을 거침. 사용자 프로세스는 이런 사실을 의식하지 않고 사용하게됨.
  * mmap 함수로 메모리를 확보하는 것은 '가상 메모리를 확보했음'을 의미하고 가상 메모리에 접근하여 물리 메모리를 확보하고 매핑하는 것을 '물리 메모리를 확보했음'이라고 함. 따라서 mmap이 성공하더라도 물리 메모리가 충분하지 않으면 물리 메모리 부족이 발생할 수 있음.
  * 실험 : p154
  * `sar -B 1` 로 시스템의 페이지 폴트 현황을 1초마다 볼 수 있음
  * 프로세스 별로 보기 : ps -eo pid,comm,vsz,rss,maj_flt,min_flt
    * vsz : 가상 메모리
    * rss : 물리 메모리
    * maj_flt + min_flt = 페이지 폴트 수 (메이저 폴트 수 + 마이너 폴트 수)
* 가상 메모리 부족과 물리 메모리 부족
  * 가상 메모리 부족 : 가상 주소 공간을 모두 사용하고 있어 물리 메모리가 남아 있는지 여부와 관계없이 가상 메모리를 확보할 수 없는 상태. x86 은 가상 주소 공간 4GB 였음. x86_64 는 128TB.
  * 물리 메모리 부족 : 말그대도 시스템에 탑재된 물리 메모리가 부족한 상태. 이 또한 가상 메모리가 얼마나 남아 있는지와 상관없이 발생 가능.
* Copy on Write (CoW)
  * fork 시스템콜은 가상 메모리 방식을 사용해서 고속화됨.
  * fork를 수행할 때 부모 프로세스의 메모리를 자식 프로세스에 전부 복사하는것이 아니고 페이지 테이블만 복사함.
    * 즉, 메모리 100MB 쓰는 프로세스를 fork한다고 즉각 자식 프로세스가 100MB를 먹는게 아니라 페이지 테이블 사이즈만큼만 메모리를 더 사용하게 됨.
  * 이 때, 부모와 자식 프로세스의 전체 페이지에 쓰기 권한을 무효화함.
  * 따라서 읽기만 한다면 부모 자식 모두 공유된 물리 메모리에 접근할 수 있음.
  * 하지만 어느 한 쪽이 페이지를 변경하려고 하면(쓰기) 공유를 해제함
    1. 페이지에 쓰기가 금지된 상태이므로 CPU에 페이지 폴트 발생
    2. CPU가 커널 모드로 변경되어 커널의 페이지 폴트 핸들러가 동작
    3. 페이지 폴트 핸들러는 접근한 페이지를 다른 장소에 복사하고, 쓰려고 한 프로세스에 할당한 후 내용을 다시 작성
    4. 부모 프로세스, 자식 프로세스 각각 공유가 해제된 페이지에 대응하는 페이지 테이블 엔트리 업데이트. 공유가 해제되었으므로 어느 쪽에서도 해당 엔트리에 쓰기가 가능해짐.
  * CoW의 원리에 의해 fork가 성공하더라도 페이지 폴트 핸들러가 동작하는 시점에 물리 메모리가 없으면 물리 메모리 부족이 발생할 수 있음.
  * 물리 메모리 사용량의 경우 부모와 자식의 메모리 중 공유된 부분은 각각 프로세스에 이중으로 계산됨. 이 때문에 전체 프로세스의 물리 메모리 사용량을 모두 더하면 전체 프로세스가 실제로 사용하고 있는 메모리의 양보다 더 큰 값을 가지게 됨.
* 스왑(swap)
  * 스왑은 저장 장치의 일부를 일시적으로 메모리 대신 사용하는 방식
  * OOM에 대처하는 구제 장치라 할 수 있꼬 가상 메모리 방식을 응용한 것임.
  * Windows에서의 스왑 영역은 '가상 메모리'를 의미함으로 헷갈리지 말자.
  * 임시 보관한 페이지의 스왑 영역의 위치는 커널 내의 테이블 엔트리와는 별도로 스왑 영역 관리용 장소에 기록된다.
  * 물리 메모리에 빈 공간이 없으면 커널은 사용 중인 물리 메모리의 일부를 스왑 영역에 임시 보관한다. 이를 `swap out` 이라 한다. 스왑 아웃할 영역은 커널이 특정한 알고리즘을 통해 결정한다.
  * 커널이 스왑 영역에 임시 보관했던 데이터를 물리 메모리에 되돌리는 것을 `swap in` 이라 한다.
  * 스왑 인, 스왑 아웃을 합쳐 스와핑 (swapping) 이라 부른다. 리눅스는 스왑의 단위가 페이지 단위 이므로 페이징(paging) 이라고도 부른다. 이 꼉우 스왑 인, 스왑 아웃도 페이지 인, 페이지 아웃이라 부른다.
  * 저장 장치는 메모리에 비하면 매우 느리며 시스템의 메모리 부족이 만성적으로 부족하면 메모리에 접근할 때마다 스와핑을 반복하는데 이를 스래싱(thrashing) 이라 한다. 스래싱이 나타나면 시스템이 사용자 입력에 한동안 반응하지 못 하거나 결국 컴퓨터가 멈추거나 메모리가 부족하게 된다. 따라서 스와핑은 발생하지 않는게 최선이다.
  * 스왑 확인
    * `swapon --show`
    * `free` 의 Swap: 영역
    * `sar -W 1` 1초마다 시스템의 스왑인, 스왑아웃의 수 출력. (pswpin/s : 1초당 스왑 인 된 페이지 수, pswpout/s : 1초당 스왑 아웃된 페이지 수)
    * `sar -S 1` : 1초마다 스왑 영역의 사이즈 확인. `kbswpused` 필드값이 증가하고 있으면 위험한 것.
  * Major Fault : 스와핑과 같이 저장 장치에 대한 접근이 발생하는 페이지 폴트.
  * Minor Fault : 저장 장치 접근이 발생하지 않는 페이지 폴트
* 계층형 페이지 테이블
  * x86_64 아키텍처의 가상 주소 공간 크기는 128TB, 1페이지 크기 4KB, 페이지 테이블 엔트리 사이즈 8KB
  * 페이지 테이블을 1:1로 단순 매핑해서 만들면 프로세스 1개당 페이지 테이블 사이즈로 256GB가 필요함. (8bytes * 128TB / 4KB)
  * 이를 해결하기 위해 계층구조로 페이지 테이블을 생성하여 페이지 테이블의 메모리 사용량을 절약함. x86_64는 4계층 구조라고 함.
  * 물리 메모리 중 페이지 테이블 자체가 사용하는 양은 `sar -r ALL` 의 `kbpgtbl` 필드로 확인
  * 페이지 테이블 자체도 메모리를 사용하므로 메모리 부족의 이유는 프로세스가 직접 사용하는 물리 메모리양의 증가 뿐만 아니라 '프로세스를 너무 많이 만들었다' 거나 '가상 메모리를 대량으로 사용하는 프로세스 때문에' 페이지 테이블 영역이 커져서인 경우도 있을 수 있다.
    * 전자는 프로세스의 수를 줄이는 방법을 검토하고
    * 후자는  Huge Page로 해결한다.
* Huge Page
  * 프로세스가 사용하는 가상 메모리 사이즈가 증가하면 페이지 테이블 자체의 물리 메모리양도 증가한다.
  * 이 경우 메모리 사용량 증가는 물론이고 fork 시스템 콜도 느려진다. Copy On Write 방식으로 최소한의 메모리만 복사하지만 페이지 테이블 자체는 부모 프로세스와 같은 사이즈로 새로 복사해야하기 때문이다.
  * Huge Page는 이름 대로 커다란 사이즈의 페이지이며 프로세스의 페이지 테이블에 필요한 메모리의 양을 줄일 수 있다.
  * p182를 보면 한 개의 페이지 테이블 엔트리가 매핑하는 단위 자체를 크게 해서 페이지 테이블 엔트리 수를 줄임으로써 페이지 테이블 자체의 용량을 줄이는 기법이다. 아무튼 가상 메모리를 많이 쓰는 프로세스에 대해 페이지 테이블에 필요한 메모리의 양을 줄이기 위한 기법이라는 것만 기억하자.
  * 리눅스는 Transparent Huge Page 라는 기능이 있다. 가상 주소 공간에 연속된 여러 개의 4KB 페이지가 특정 조건을 만족하면 이것을 묶어 자동으로 Huge Page로 바꿔주는 기능이다. 언뜻 좋아보이지만 Huge Page로 묶는 처리, 조건이 더 이상 만족하지 않아 다시 4KB 페이지로 풀어야할 경우의 처리가 있어 국소적으로 성능이 하락하는 경우도 있다.
    * `/sys/kernel/mm/transparent_hugepage/enabled` 파일에서 확인 가능. (우분투 20.04에서 볼 때 기본 활성화)
    * always가 활성화. 파일에 never 라고 덮어씌우면 비활성화. madvise는 madvise() 시스템 콜을 사용하여 특정 메모리 영역에만 이 기능을 사용하도록 함.



## chap06 메모리 계층

* 메모리 장치의 계층 구조
  * 저장 장치 <-> 메모리 <-> 캐시 메모리 <-> 레지스터



### 캐시 메모리

* 메모리와 레지스터 사이에서 둘의 속도 차이에 의한 병목을 최소화하기 위한 메모리
* 읽기
  * CPU가 읽고자 하는 메모리 영역을 캐시 라인 사이즈 단위로 캐시 메모리에 읽고 CPU는 캐시 메모리를 읽는다. 이후 같은 영역을 접근할 때는 메모리 접근 없이 바로 캐시 메모리에 접근한다.
* 쓰기
  * 레지스터에서 메모리 쓰기가 발생하면 레지스터에서 변경한 결과를 캐시 라인 사이즈단위로 캐시 메모리에 데이터가 변경되었음을 나타내는 플래그를 표시한다. 플래그가 표시된 캐시 라인을 더티라고 부른다. 이후 백그라운드처리로 더티 상태의 캐시 라인이 메모리에 기록된다.
    * 백그라운드로 메모리에 쓰는 방식을 write back
    * 캐시 라인이 더티가 된 순간 메모리에 즉시 써 넣는 방식을 write through 방식이라 함
* 캐시 메모리가 가득찬 상태에서 새 영역을 읽으면 기존 캐시 중에 적절히 골라서 파기한 뒤 읽기를 수행함.
  * 파기하는 캐시가 더티면 메모리에 쓰기를 수행한 뒤 동기화 작업이 발생함. 캐시 메모리가 가득 차고 모든 캐시 라인이 더티라면 메모리 접근을 할 때마다 캐시 라인 안의 데이터가 자주 바뀌게 되는 스래싱(thrashing) 이 발생해 성능이 저하할 수 있음
* 캐시 메모리는 L1, L2, L3 등의 계층 구조로 구성되며 숫자가 작을수록 용량이 작고 빠름
* 메모리 참조성의 국소성이라는 특징 때문에 프로세스는 짧은 시간을 놓고 생각해보면 자신이 획득한 메모리의 총량보다 훨씬 좁은 범위에 메모리에 접근하는 성향이 있음. 이 좁은 범위를 캐시 메모리 사이즈가 커버할 수 있으면 성능이 좋은 것.
  * 시간 국소성 : 특정 시점에 접근하는 데이터는 가까운 미래에 다시 접근할 가능성이 크다. 루프 처리 중인 코드 영역 등.
  * 공간 국소성 : 특정 시점에 어떤 데이터에 접근하면 그 데이터와 가까운 주소에 있는 데이터를 접근할 가능성이 높다. 배열의 전체 검색 등.
  * 속도를 중요시하는 프로그램이라면 메모리 참조의 국소성을 고려하여 단위 시간 당 메모리 접근 범위를 캐시 메모리 사이즈에 맞게 조정하는 것이 필요하다.
* Translation Lookaside Buffer (TLB)
  * 메모리 접근은 크게보면 1. 물리 메모리상에 존재하는 페이지 테이블에서 가상 주소를 물리주소로 변환하고 2. 1.에서 구한 메모리에 접근 이라는 순서로 발생함.
  * 캐시 메모리로 고속화되는건 2. 번임. 1.의 과정은 어쨋든 물리 메모리에 접근해야하기 때문.
  * 1.을 고속화하기 위한 영역이 TLB



### 페이지 캐시

* 저장장치 내의 파일 데이터를 메모리 캐싱하여 속도 차이를 줄이는 것.
* 읽기 : 프로세스가 파일을 읽으면 먼저 커널의 메모리 내에 있는 페이지 캐시 영역에 복사한 뒤 이 데이터를 프로세스 메모리에 복사함. 페이지 캐시에 올라온 파일을 다시 접근할 때는 바로 캐시 메모리를 읽으므로 빠름. 또 페이지 캐시는 전체 프로세스 공유 자원이므로 다른 프로세스도 같은 효과를 얻음
* 쓰기 : 프로세스가 데이터를 파일에 쓰면 커널은 페이지 캐시에 데이터를 쓴다. 이 때 커널은 데이터에 대응되는 엔트리에 '데이터의 내용은 저장 장치의 내용보다 새로운 것' 이라는 플래그를 붙이는데 이런 페이지를 더티 페이지(dirty page) 라 함. 더티 페이지는 나중에 커널이 백그라운드로 처리하여 스토리지 내의 파일에 반영함. 
* 프로세스가 접근하는 데이터가 모두 페이지 캐시에 있으면 시스템 파일의 접근 속도는 메모리 접근 속도에 근접하므로 시스템 전체가 빨라짐.
* 메모리가 부족해지면 커널은 페이지 캐시를 해제함. 더티 페이지가 아닌걸 우선 해제하고 그래도 모자라면 더티 페이지를 스토리지로 라이트 백한 뒤 파기함. 메모리 부족이 더티 페이지의 라이트 백을 자주 발생시켜 시스템이 느려질 수 있음
* open 시스템 콜로 파일을 읽을 때 `O_SYNC` 플래그를 설정하면 write 시스템콜을 수행할 때마다 페이지 캐시 외에도 저장 장치로도 바로 쓰기가 수행됨
* 버퍼 캐시 : 페이지 캐시랑 비슷. 파일시스템을 사용하지 않고 디바이스 파일을 이용하여 저장 장치에 직접 접근하는 목적으로 사용. 페이지 캐시와 버퍼 캐시를 합쳐 저장 장치의 안의 데이터를 메모리에 넣어두는 방식.



* 실험

```bash
# testfile 이라는 1GB 파일 만들기
# oflag=direct : Direct I/O 사용. 쓰기에 페이지 캐시를 사용하지 않음.
dd if=/dev/zero of=testfile oflag=direct bs=1M count=1K

# 메모리 영역 확인 buff/cache 영역 확인.
free

# user+sys의 합보다 real이 몇 배는 크게 나온다. I/O에 대부분의 시간이 사용되었음을 알 수 있다.
time cat testfile > /dev/null

# cat하면서 페이지 캐시가 생겨서 buff/cache 가 늘어난다.
free

# 페이지캐시에서 바로 읽어오므로 user+sys의 합계가 real에 근접한다.
time cat testfile > /dev/null

# kbcached 필드로도 확인 가능
sar -r 1 1

# oflag=direct 가 없으면 페이지 캐시에만 쓰므로 더 빠르다. (근데 VM으로 하니까 oflag 있는게 더 빨랐음.)
time dd if=/dev/zero of=testfile bs=1M count=1K
```

* P214 에 페이지 캐시 제어를 위한 튜닝 파라미터가 있으니 참고. sysctl 파라미터 튜닝을 통해 페이지 캐시의 라이트 백 발생을 조정하여 I/O 부하를 줄이는 것이 포인트.



### 하이퍼스레드

* 메모리도 느리지만 캐시메모리도 CPU 계산 처리에 비하면 느림
* time 명령의 user나 sys가 나타내는 CPU 사용 시간 중 대부분은 메모리, 캐시 메모리로부터 데이터를 기다리는 일로 낭비됨
* 이런 대기 시간으로 낭비는 자원을 유효하게 활용하기 위한 것이 하이퍼스레드
* 하이퍼스레드는 CPU 코어 안의 레지스터 등 일부 자원을 여러 개 준비하고 시스템 입장에서는 각각 논리 CPU 로 인식되는 하이퍼스레드라는 단위로 분할되는 하드웨어의 기능임. 특정 조건 아래에서만 여러개가 동시에 실행 가능함.
* 항상 좋은건 아니고 베스트 케이스라해도 20~30% 정도 성능 향상이 나오면 훌륭하다고 봄. 조건에 따라 느려지는 경우도 있음. 따라서 사용 여부는 실제 성능 테스트를 통해 결정해야함.



## chap07 파일시스템

* 저장장치의 기능을 단순한게 말하면 '저장 장치 안에 지정된 주소에 대해 특정 사이즈의 데이터를 읽거나 씀'
* 파일시스템이 없다면 사용자는 어느 주소에 어떤 용량의 파일을 쓸 것인지를 직접 요청해야하고 또한 이 위치를 직접 기억해야함. 이런 복잡한 처리를 피하고자 어디에 어느 정도의 데이터가 있는지, 어디가 빈 영역인지를 관리하는 방법이 파일시스템. (이름, 위치, 사이즈 등의 보조 정보)
* ext4, XFS, Btrfs 등의 다양한 파일시스템 지원. 파일시스템마다 다룰 수 있는 파일의 사이즈, 파일시스템의 사이즈, 처리 속도 등이 다름. 하지만 프로세스 입장에서는 공통된 시스템 콜로 호출할 수 있음.
  * 파일 작성, 삭제 : create(), unlink()
  * 파일 열기, 닫기 : open(), close()
  * 파일의 특정 위치로 이동 : lseek()
  * 등
  * 이런 시스템 콜이 호출되는 파일시스템 공통 처리를 수행하고 ext4와 같은 각 파일시스템별 처리를 수행한 뒤 실제 저장장치의 접근은 디바이스 드라이버를 통해 수행한다.



### 용량 제한

* 파일시스템의 용량을 용도별로 제한할 수 있는 기능을 일반적으로 쿼터(quota) 라 부름
  * 사용자 쿼터 : 파일의 소유자인 사용자별로 용량을 제한 (ext4, XFS)
  * 디렉터리 쿼터 : 특정 디렉토리별로 용량 제한 (ext4, XFS)
  * 서브 볼륨 쿼터 : 파일시스템 내의 서브 볼륨이라는 단위별 용량 제한



### 파일시스템이 깨진 경우

* 파일시스템의 데이터를 쓰다가 시스템의 전원이 끊기는 등의 케이스로 인해 파일시스템의 내용이 깨지는 경우
* 예를 들어 디렉토리를 이동(mv) 한다면 새로운 부모링크가 바라보도록 하고 기존 부모의 링크는 끊는 두 가지 처리를 하게됨. 당연히 이는 atomic한 처리임. 여기서 첫 번째 작업 직후 시스템 전원이 나가면 재부팅 후 파일시스템은 깨진 상태가 됨. 
* 파일 시스템이 깨지는걸 방지하는 기술은 저널링(ext4, XFS)과 Copy On Write(Btrfs)가 대표적임.



* 저널링
  * DBMS의 binlog 같은 느낌일까? 이벤트 기반이라 볼 수도 있을 듯 하고 독립적인 시스템간의 통합 트랜잭션처리 이용할 수 있어보인다.
  * 파일시스템 내에 저널 영역이라는 특수 영역을 준비함. 이 영역에 아토믹한 처리의 목록을 기록함 (이를 저널로그라 함) 이후 저널 영역의 내용을 바탕으로 실제로 파일시스템의 내용을 업데이트함. 업데이트 완료 후에 저널 영역 파일 제거.
  * case1 : 저널로그 기록중 시스템 중단
    * 재부팅후 저널로그만 제거함. 시스템은 처리 전 상태와 같으므로 문제없음.
  * case2 : 저널로그를 기반으로 업데이트중 시스템 중단
    * 저널로그가 아직 있으므로 저널로그를 처음부터 다시 실행하고 이후 저널 영역을 지우고 완료.
* Copy On Write (p235)
  * ext4, XFS 등의 파일시스템은 일단 파일을 작성하면 그 파일의 배치 장소가 원칙적으로 바뀌지 않음. 파일의 내용을 업데이트할 때마다 저장 장치상의 같은 장소에 새로운 데이터를 써넣음.
  * Btrfs 등의 Copy On Write 형 파일시스템은 일단 파일을 작성하더라도 업데이트할 때 다른 장소에 데이터를 씀.
  * 업데이트되는 데이터를 다른 장소에 전부 쓴 뒤에 링크를 고쳐쓰는 방식으로 동작함. 고쳐쓰는 중에 강제 전원 단절이 발생해도 재부팅 후 작성하던 데이터만 지우면 원래 상태로 복구할 수 있음
* 그 외의 방법
  * 정기적으로 파일시스템을 백업하여 데이터 손실을 대비한다.
  * fsck 와 같은 깨진 파일시스템을 복구하는 명령어도 존재한다. 하지만 매우 많은 시간이 걸리고 실패하는 경우도 잦으며 사용자가 원하는 상태로 복구된다는 보장도 없으므로 권장되지 않는다.



### 파일의 종류

* 리눅스는 하드웨어상의 장치를 거의 모두 파일로서 표현한다. (네트워크 어댑터는 예외로 장치에 대응되는 파일이 없음)
* 따라서 리눅스는 장치를 파일과 동등하게 open, read, write 등의 시스템콜로 제어할 수 있다.
* 장치 고유의 복잡한 동작에는 ioctl 시스템콜을 사용한다.
* 각 디바이스 파일은 /dev 에 위치한다.

```bash
ls -l /dev

# c로 시작하면 캐릭터 장치
crw-rw-rw-  1 root tty       5,   0 Jul  6 13:37 tty
# b로 시작하면 블록 장치
brw-rw----  1 root disk      8,   2 Jul  6 13:37 sda2
```



* 캐릭터 장치
  * read, write 가 가능하지만 seek는 되지 않는 특성이 있음
  * 터미널, 키보드 마우스 등
  * 예를 들어 터미널의 디바이스 파일에서 write는 터미널에 데이터를 출력하고, read는 터미널에 데이터를 입력함 

```bash
ps -ax | grep bash
# 두번째 필드 pts/0 와 같이 현재 터미널에 대응하는 장치를 얻어낸뒤
echo hello > /etc/pts/0 
# 을 하면 현재 터미널에 출력된다. 내부적으로는 디바이스 파일에 write 시스템콜을 요청한 것
# 응용하면 열려있는 다른 터미널에도 출력할 수 있다.
```

* 블록 장치
  * 단순 읽기, 쓰기 이외에 랜덤 접근이 가능.
  * HDD, SDD 등의 저장 장치가 대표적.
  * 블록 장치는 일반적으로 직접 접근하지 않고 거기에 파일시스템을 작성해서 그것을 마운트함으로써 파일시스템을 경유해서 사용함.
    * 블록 장치 레벨의 데이터 백업&복구(dd 명령어 등), 파일시스템의 마운트(mount), 파일시스템작성 (파일 시스템별 mkfs 명령어) 등이 블록 장치를 직접 다루는 경우
  * p243 예제에서는 파일시스템을 마운트하여 파일을 쓴 뒤 언마운트하고 블록장치파일에서 해당 파일의 위치를 찾아서 dd 명령어로 바꾼뒤 다시 마운트하면 파일의 내용이 dd 명령어로 바꾼채로 남아있음을 확인할 수 있다. (ubuntu20-04에서도 잘 됐다.) 이 예제로 블록 장치를 조작해 저장 장치를 다룰 수 있다는 것과 파일시스템도 내용을 열어보면 단지 저장 장치에 배치된 데이터에 불과함을 알 수 있다.





## 소감

* 당장 어디에 써먹을 수는 없으나 리눅스의 기본 동작 원리 파악에 도움이 됨. 좀 더 low level로 생각할 수 있도록 해주는 책.
* OS적인 트러블슈팅을 할 때 큰 도움이 될 듯.